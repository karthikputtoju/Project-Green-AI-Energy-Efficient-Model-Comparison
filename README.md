![Python](https://img.shields.io/badge/Python-3.9%2B-blue?style=flat&logo=python)
![Machine Learning](https://img.shields.io/badge/Machine%20Learning-ML-brightgreen?style=flat&logo=tensorflow)
![Generative AI](https://img.shields.io/badge/Generative%20AI-Green%20AI%20Project-orange?style=flat&logo=openaigym)
![License](https://img.shields.io/badge/License-MIT-blue.svg)

# Green AI â€“ Energy Efficient Model Comparison

This project demonstrates a real-world application of **Green AI** by comparing two popular Hugging Face models:

- **DistilGPT2** â€“ A lightweight, distilled version of GPT2, optimized for speed and energy efficiency.  
- **GPT2** â€“ The original large-scale transformer model by OpenAI.

---

## Objective

To **quantify and compare**:

- **Energy consumed**
- **COâ‚‚ emissions**
- **Generation time**
- **Output length**

â€¦while generating the same prompt using both models.

---

## Why Green AI?

AI workloads consume increasing amounts of **energy** and produce **COâ‚‚ emissions**.  
Green AI focuses on **sustainable AI practices** by:

- Reducing model size
- Optimizing computation
- Tracking environmental impact

This project **proves** that smaller models can often give **similar results** while having a lower **carbon footprint**.

---

## Project Structure

```
green-ai-model-comparison/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ green_ai_comparison.py # Main script
â”œâ”€â”€ outputs/ # Generated text outputs
â”‚ â”œâ”€â”€ distilgpt2_output.txt
â”‚ â””â”€â”€ gpt2_output.txt
â”œâ”€â”€ metrics/ # Performance & emissions metrics
â”‚ â”œâ”€â”€ distilgpt2_emissions.csv
â”‚ â”œâ”€â”€ gpt2_emissions.csv
â”‚ â””â”€â”€ performance_metrics.json
â”‚ â””â”€â”€ chart.py
```

##  Installation & Setup

1ï¸âƒ£ **Clone this repo**
```bash
git clone https://github.com/karthikputtoju/green-ai-model-comparison.git
cd green-ai-model-comparison
```

2ï¸âƒ£ Create a virtual environment
```
python -m venv venv
source venv/bin/activate  # Mac/Linux
venv\Scripts\activate     # Windows
```

3ï¸âƒ£ Install dependencies
```
pip install -r requirements.txt

```
Note: If any challenges faced, install respective packages manually.

â–¶ï¸ Run the Comparison
```
python green_ai_comparison.py

```
This will:
- Run DistilGPT2 & GPT2 on the same prompt
- Track energy usage, COâ‚‚ emissions, and inference time with CodeCarbon

Save:
- Generated text â†’ outputs/
- Metrics in JSON & CSV â†’ metrics/
- Color-coded performance chart â†’ metrics/

## Example Results
metrics/performance_metrics.json:
```
{
    "distilgpt2": {
        "Energy consumption (kWh)": 0.0,
        "COâ‚‚ emissions (kg)": 9.9e-05,
        "Inference time (s)": 14.182,
        "Output quality (chars)": 1214
    },
    "gpt2": {
        "Energy consumption (kWh)": 0.0,
        "COâ‚‚ emissions (kg)": 7.1e-05,
        "Inference time (s)": 10.274,
        "Output quality (chars)": 1280
    }
}
```

## Calculation Results with using Formulas
```
1. Energy Consumption (kWh)
---------------------------
Energy used is calculated from the hardware power draw and inference runtime:

EnergyÂ (kWh) = PowerÂ (Watts) Ã— TimeÂ (seconds) / 1000 Ã— 3600	â€‹
Power (Watts): GPU/CPU power usage while running the model.
Time (seconds): How long inference took.
1000Ã—3600 to convert from Joules â†’ kWh.

Example for GPT2:
If GPU uses ~45W and runs for 8.625s:

Energy = 45 Ã— 8.625 / 3,600,000 â‰ˆ 0.000126 kWh

2. COâ‚‚ Emissions (grams)
------------------------
Once energy consumption is known, COâ‚‚ emissions are derived using the Carbon Intensity Factor of electricity (varies by country, avg. ~0.475 g per Wh).

COâ‚‚Â (g) = EnergyÂ (kWh) Ã— 1000 Ã— CarbonÂ IntensityÂ (g/Wh)
Energy in kWh â†’ convert to Wh by multiplying with 1000.
Multiply by COâ‚‚ factor (g/Wh).

Example for DistilGPT2:
0.000100 kWh Ã— 1000 Ã— 0.47 â‰ˆ 0.047gÂ COâ‚‚

3. Inference Time (seconds)
---------------------------
This is measured directly using a timer:

InferenceÂ Time = EndÂ Time âˆ’ StartÂ Time

For DistilGPT2 â†’ 6.834s
For GPT2 â†’ 8.625s

4. Output Quality (chars)
-------------------------
This is a proxy metric â†’ counts how many characters were generated in the modelâ€™s response.

OutputÂ QualityÂ (chars) = LengthÂ ofÂ generatedÂ textÂ inÂ characters

DistilGPT2 â†’ 1214 chars
GPT2 â†’ 1280 chars

```

## Visual Comparison
- ğŸŸ¢ Good â€“ Best performer for that metric
- ğŸŸ§ Better â€“ Mid performer
- ğŸ”´ Bad â€“ Lowest performer
<img width="1886" height="1060" alt="image" src="https://github.com/user-attachments/assets/d57f4d9a-fe68-49b4-b8fb-7ec2afb63a96" />


## Key Learnings
- Smaller models can use less energy, but not always faster on every run.
- Energy use & COâ‚‚ emissions are measurable in real-time.
- Tracking environmental impact is key for Green AI adoption in ML projects.

---
## Author
**Karthik Puttoju**  
https://github.com/karthikputtoju

## License
This project is licensed under the MIT License â€“ free to use and if anyone want to contribute to this project, please reach out to me.

## Acknowledgments
- Hugging Face Transformers
- CodeCarbon
- Inspired by the growing Green AI movement in ML research.

---
